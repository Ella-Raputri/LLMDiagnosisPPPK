{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4690b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Download required NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2fcbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+%', '', text)  # Remove percentages\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ddd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two texts\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if pd.isna(text1) or pd.isna(text2) or text1 == \"\" or text2 == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate BLEU score between two texts\n",
    "def calculate_bleu_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate_text.lower())\n",
    "    \n",
    "    # Apply smoothing function for short texts\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    try:\n",
    "        # Calculate BLEU score with different n-gram weights\n",
    "        bleu1 = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "        bleu2 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "        bleu3 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "        bleu4 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "        \n",
    "        # Average BLEU scores\n",
    "        avg_bleu = (bleu1 + bleu2 + bleu3 + bleu4) / 4\n",
    "        return avg_bleu\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate METEOR score between two texts\n",
    "def calculate_meteor_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate_text.lower())\n",
    "    \n",
    "    try:\n",
    "        return meteor_score([reference_tokens], candidate_tokens)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate BERT score\n",
    "def calculate_bert_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode texts\n",
    "    inputs1 = tokenizer(reference_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs2 = tokenizer(candidate_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs1 = {k: v.to(device) for k, v in inputs1.items()}\n",
    "    inputs2 = {k: v.to(device) for k, v in inputs2.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "    \n",
    "    # Use CLS token embeddings for sentence representation\n",
    "    embeddings1 = outputs1.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    embeddings2 = outputs2.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Calculate cosine similarity between embeddings\n",
    "    similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3b25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved function to standardize disease names\n",
    "def standardize_disease_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    \n",
    "    name = str(name).lower().strip()\n",
    "    # Remove percentages\n",
    "    name = re.sub(r'\\d+%', '', name).strip()\n",
    "    \n",
    "    # Common variations of disease names to standardize - EXPANDED\n",
    "    mapping = {\n",
    "        # Demam (Fever) related\n",
    "        'dbd': 'demam berdarah dengue',\n",
    "        'dengue fever': 'demam berdarah dengue',\n",
    "        'demam dengue': 'demam berdarah dengue',\n",
    "        'dengue': 'demam berdarah dengue',\n",
    "        'demam berdarah': 'demam berdarah dengue',\n",
    "        'demam biasa': 'common fever',\n",
    "        'common fever': 'common fever',\n",
    "        'demam umum': 'common fever',\n",
    "        \n",
    "        # Gastro related\n",
    "        'gastroenteritis akut': 'gastroenteritis',\n",
    "        'gastroenteritis (ge) akut': 'gastroenteritis',\n",
    "        'ge akut': 'gastroenteritis',\n",
    "        'gastroenteritis': 'gastroenteritis',\n",
    "        'diare akut': 'gastroenteritis',\n",
    "        \n",
    "        # Respiratory related\n",
    "        'infeksi saluran pernapasan atas': 'ispa',\n",
    "        'ispa': 'ispa',\n",
    "        'infeksi saluran napas atas': 'ispa',\n",
    "        'infeksi saluran pernafasan atas': 'ispa',\n",
    "        'infeksi saluran pernapasan': 'ispa',\n",
    "        \n",
    "        # GERD related\n",
    "        'reflux gastroesofagus': 'gerd',\n",
    "        'reflux asam lambung': 'gerd',\n",
    "        'refleks asam lambung': 'gerd',\n",
    "        'gastroesophageal reflux disease': 'gerd',\n",
    "        'refleks gastroesofagus': 'gerd',\n",
    "        'gerd': 'gerd',\n",
    "        \n",
    "        # Gastritis related\n",
    "        'maag': 'gastritis',\n",
    "        'gastritis akut': 'gastritis',\n",
    "        'penyakit maag': 'gastritis',\n",
    "        'penyakit maag akut': 'gastritis',\n",
    "        'gastritis': 'gastritis',\n",
    "        \n",
    "        # Heart related\n",
    "        'infark miokard akut': 'serangan jantung',\n",
    "        'serangan jantung': 'serangan jantung',\n",
    "        'angina pektoris': 'angina',\n",
    "        'angina': 'angina',\n",
    "        \n",
    "        # Asthma related\n",
    "        'asma bronkial': 'asma',\n",
    "        'asma exacerbation': 'asma',\n",
    "        'asma': 'asma',\n",
    "        'pemburukan asma': 'asma',\n",
    "        \n",
    "        # Bronchitis related\n",
    "        'bronkitis akut': 'bronkitis',\n",
    "        'bronkitis': 'bronkitis',\n",
    "        \n",
    "        # Wound related\n",
    "        'vulnus laceratum': 'luka robek',\n",
    "        'luka robek': 'luka robek',\n",
    "        'luka terbuka': 'luka robek',\n",
    "        'laceration': 'luka robek',\n",
    "        'vulnus excoriatum': 'luka lecet',\n",
    "        'luka lecet': 'luka lecet',\n",
    "        \n",
    "        # Head injury related\n",
    "        'cedera kepala ringan': 'ckr',\n",
    "        'kepala cedera ringan': 'ckr',\n",
    "        'ckr': 'ckr',\n",
    "        \n",
    "        # Dyspepsia related\n",
    "        'dispepsia': 'dispepsia',\n",
    "        'dispepsia fungsional': 'dispepsia',\n",
    "        \n",
    "        # Appendicitis related\n",
    "        'appendisitis akut': 'appendisitis',\n",
    "        'appendisitis': 'appendisitis',\n",
    "        'apendisitis': 'appendisitis',\n",
    "        \n",
    "        # UTI related\n",
    "        'infeksi saluran kemih': 'isk',\n",
    "        'isk': 'isk',\n",
    "        'infeksi saluran kemih akut': 'isk',\n",
    "        \n",
    "        # Additional mappings\n",
    "        'intoleransi laktosa': 'intoleransi laktosa',\n",
    "        'pneumonia': 'pneumonia',\n",
    "        'hipertensi': 'hipertensi',\n",
    "        'hipertensi akut': 'hipertensi',\n",
    "        'hipertensi stage 1': 'hipertensi',\n",
    "        'luka bakar': 'luka bakar',\n",
    "        'burn injury': 'luka bakar',\n",
    "        'tonsilitis': 'tonsilitis',\n",
    "        'faringitis': 'faringitis',\n",
    "        'vertigo': 'vertigo',\n",
    "        'benign paroxysmal positional vertigo': 'vertigo',\n",
    "        'bppv': 'vertigo',\n",
    "        'influenza': 'influenza',\n",
    "        'kolik renal': 'kolik renal',\n",
    "        'batu ginjal': 'batu ginjal',\n",
    "        'peritonitis': 'peritonitis'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping if available - using partial matching for more flexibility\n",
    "    for key, value in mapping.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    \n",
    "    # If no mapping found, return the cleaned name\n",
    "    return name\n",
    "\n",
    "# Function to extract percentage from diagnosis text\n",
    "def extract_percentage(text):\n",
    "    match = re.search(r'(\\d+)%', text)\n",
    "    return float(match.group(1))/100 if match else 0.5  # Default to 50% if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd8ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse diagnoses from raw answer text\n",
    "def parse_diagnoses(answer_text):\n",
    "    if pd.isna(answer_text):\n",
    "        return []\n",
    "    \n",
    "    # Split by semicolon and process each diagnosis\n",
    "    diagnoses = []\n",
    "    percentages = []\n",
    "    \n",
    "    for item in answer_text.split(';'):\n",
    "        item = item.strip()\n",
    "        if not item:\n",
    "            continue\n",
    "            \n",
    "        # Extract diagnosis name and percentage \n",
    "        match = re.search(r'(.*?)(?:\\s+(\\d+)%)?$', item)\n",
    "        if match and match.group(1).strip():\n",
    "            diagnoses.append(standardize_disease_name(match.group(1).strip()))\n",
    "            percentages.append(extract_percentage(item))\n",
    "    \n",
    "    # Create a list of tuples (diagnosis, percentage)\n",
    "    return list(zip(diagnoses, percentages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41068d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the best matching answer for a given Claude answer\n",
    "def select_best_answer(answers, claude_diagnoses):\n",
    "    \"\"\"\n",
    "    Selects the best matching Qwen answer based on overlap with Claude diagnoses\n",
    "    \n",
    "    Args:\n",
    "        answers: List of Qwen answer strings\n",
    "        claude_diagnoses: List of (diagnosis, score) tuples from Claude\n",
    "    \n",
    "    Returns:\n",
    "        best_answer: The Qwen answer with highest similarity score\n",
    "    \"\"\"\n",
    "    if not answers or not claude_diagnoses:\n",
    "        return \"\"\n",
    "    \n",
    "    best_score = -1\n",
    "    best_answer = \"\"\n",
    "    \n",
    "    claude_diseases = [d[0] for d in claude_diagnoses]\n",
    "    \n",
    "    for ans in answers:\n",
    "        # Parse the current Qwen answer\n",
    "        parsed = parse_diagnoses(ans)\n",
    "        diseases = [d[0] for d in parsed]\n",
    "        \n",
    "        # Calculate overlap score - how many diseases match\n",
    "        match_count = sum(1 for d in diseases if d in claude_diseases)\n",
    "        match_score = match_count / max(len(claude_diseases), len(diseases), 1)\n",
    "        \n",
    "        # Also consider text similarity as fallback\n",
    "        text_sim = calculate_cosine_similarity(\n",
    "            preprocess_text(ans), \n",
    "            preprocess_text('; '.join([d for d, _ in claude_diagnoses]))\n",
    "        )\n",
    "        \n",
    "        # Combined score favoring matching diseases but also considering text similarity\n",
    "        combined_score = (match_score * 0.7) + (text_sim * 0.3)\n",
    "        \n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_answer = ans\n",
    "            \n",
    "    return best_answer, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4daec43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate diagnosis matching metrics\n",
    "def calculate_diagnosis_match_metrics(claude_diagnoses, diagnose):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for disease matching\n",
    "    \n",
    "    Args:\n",
    "        claude_diagnoses: List of (disease, score) tuples from Claude\n",
    "        diagnose: List of (disease, score) tuples from Qwen\n",
    "    \n",
    "    Returns:\n",
    "        dict with precision, recall, f1_score, etc.\n",
    "    \"\"\"\n",
    "    if not claude_diagnoses or not diagnose:\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'matched': 0,\n",
    "            'total_ground_truth': len(claude_diagnoses),\n",
    "            'total_predictions': len(diagnose)\n",
    "        }\n",
    "    \n",
    "    # Extract just the disease names\n",
    "    claude_diseases = [d[0] for d in claude_diagnoses]\n",
    "    diseases = [d[0] for d in diagnose]\n",
    "    \n",
    "    # Count matches\n",
    "    matches = sum(1 for d in diseases if d in claude_diseases)\n",
    "    \n",
    "    # Calculate precision: matches / qwen predictions\n",
    "    precision = matches / len(diseases) if diseases else 0\n",
    "    \n",
    "    # Calculate recall: matches / claude diagnoses (ground truth)\n",
    "    recall = matches / len(claude_diseases) if claude_diseases else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'matched': matches,\n",
    "        'total_ground_truth': len(claude_diseases),\n",
    "        'total_predictions': len(diseases)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5099cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_medical_diagnoses():\n",
    "    xlsx_path = 'evaulation/30 sample penyakit - hasil prompt LLM.xlsx'\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        claude_raw = pd.read_excel(xlsx_path, sheet_name='Claude 3.5 Haiku')\n",
    "        deepseek_raw = pd.read_excel(xlsx_path, sheet_name='Deepseek-V3-(nonRAG)')\n",
    "        \n",
    "        for df in [claude_raw, deepseek_raw]:\n",
    "            df['No'] = df['No'].ffill()\n",
    "            df['Question'] = df['Question'].ffill()\n",
    "        \n",
    "        claude_grouped = claude_raw.groupby(['No', 'Question'])\n",
    "        deepseek_grouped = deepseek_raw.groupby(['No', 'Question'])\n",
    "\n",
    "        print(deepseek_grouped)\n",
    "        \n",
    "        all_questions = list(set(claude_grouped.groups.keys()) | \n",
    "                             set(deepseek_grouped.groups.keys()) )\n",
    "        all_questions.sort(key=lambda x: float(x[0]) if x[0] is not None and not pd.isna(x[0]) else float('inf'))\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        # Evaluate for each model variant\n",
    "        for model_name, grouped_data in [('Deepseek', deepseek_grouped)]:\n",
    "            for idx, (no, question) in enumerate(all_questions):\n",
    "                try:\n",
    "                    print(f\"[{model_name}] Processing question {idx+1}/{len(all_questions)}: {no}\")\n",
    "\n",
    "                    if (no, question) not in claude_grouped.groups:\n",
    "                        print(f\"Warning: No Claude data for question {no}\")\n",
    "                        continue\n",
    "\n",
    "                    # Claude data\n",
    "                    claude_rows = claude_grouped.get_group((no, question))\n",
    "                    claude_answers = claude_rows['Answer'].dropna().tolist()\n",
    "                    claude_diagnoses = []\n",
    "                    for ans in claude_answers:\n",
    "                        claude_diagnoses.extend(parse_diagnoses(ans))\n",
    "                    claude_explanations = claude_rows['Full Answer'].dropna().tolist()\n",
    "                    claude_full_text = ' '.join(claude_explanations)\n",
    "\n",
    "                    # Model answer\n",
    "                    if (no, question) in grouped_data.groups:\n",
    "                        model_rows = grouped_data.get_group((no, question))\n",
    "                        model_answers = model_rows['Answer'].dropna().tolist()\n",
    "\n",
    "                        best_model_answer, similarity_score = select_best_answer(model_answers, claude_diagnoses)\n",
    "                        model_diagnoses = parse_diagnoses(best_model_answer)\n",
    "\n",
    "                        explanation_rows = model_rows[model_rows['Answer'] == best_model_answer]\n",
    "                        explanations = explanation_rows['Full Answer'].dropna().tolist()\n",
    "                        model_full_text = ' '.join(explanations) if explanations else \"\"\n",
    "                    else:\n",
    "                        print(f\"Warning: No {model_name} data for question {no}\")\n",
    "                        best_model_answer = \"\"\n",
    "                        similarity_score = 0.0\n",
    "                        model_diagnoses = []\n",
    "                        model_full_text = \"\"\n",
    "\n",
    "                    # Match metrics\n",
    "                    diagnosis_metrics = calculate_diagnosis_match_metrics(claude_diagnoses, model_diagnoses)\n",
    "                    claude_answer_text = '; '.join([f\"{d} {int(p*100)}%\" for d, p in claude_diagnoses])\n",
    "                    \n",
    "                    nlp_metrics = {\n",
    "                        'answer_cosine_similarity': calculate_cosine_similarity(\n",
    "                            preprocess_text(claude_answer_text), \n",
    "                            preprocess_text(best_model_answer)\n",
    "                        ),\n",
    "                        'full_answer_cosine_similarity': calculate_cosine_similarity(\n",
    "                            preprocess_text(claude_full_text), \n",
    "                            preprocess_text(model_full_text)\n",
    "                        ),\n",
    "                        'bleu_score': calculate_bleu_score(\n",
    "                            preprocess_text(claude_answer_text), \n",
    "                            preprocess_text(best_model_answer)\n",
    "                        ),\n",
    "                        'meteor_score': calculate_meteor_score(\n",
    "                            preprocess_text(claude_answer_text), \n",
    "                            preprocess_text(best_model_answer)\n",
    "                        ),\n",
    "                        'bert_score': calculate_bert_score(\n",
    "                            preprocess_text(claude_answer_text), \n",
    "                            preprocess_text(best_model_answer)\n",
    "                        )\n",
    "                    }\n",
    "\n",
    "                    # Append result\n",
    "                    result = {\n",
    "                        'Model': model_name,\n",
    "                        'No': no,\n",
    "                        'Question': question,\n",
    "                        'Claude_Diagnoses': claude_answer_text,\n",
    "                        'Selected_Model_Answer': best_model_answer,\n",
    "                        'Similarity_Score': similarity_score,\n",
    "                        **diagnosis_metrics,\n",
    "                        **nlp_metrics\n",
    "                    }\n",
    "\n",
    "                    all_results.append(result)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing question {no} for model {model_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        # Final DataFrame\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "\n",
    "        # Overall metrics per model\n",
    "        overall_metrics = results_df.groupby('Model').agg({\n",
    "            'precision': 'mean',\n",
    "            'recall': 'mean',\n",
    "            'f1_score': 'mean',\n",
    "            'answer_cosine_similarity': 'mean',\n",
    "            'full_answer_cosine_similarity': 'mean',\n",
    "            'bleu_score': 'mean',\n",
    "            'meteor_score': 'mean',\n",
    "            'bert_score': 'mean'\n",
    "        }).reset_index()\n",
    "        overall_metrics['total_questions'] = results_df.groupby('Model')['No'].nunique().values\n",
    "\n",
    "        return results_df, overall_metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel file: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86ee91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting medical diagnosis evaluation...\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000002AE7D99B190>\n",
      "[Deepseek] Processing question 1/50: 1.0\n",
      "[Deepseek] Processing question 2/50: 2.0\n",
      "[Deepseek] Processing question 3/50: 3.0\n",
      "[Deepseek] Processing question 4/50: 4.0\n",
      "Warning: No Claude data for question 4.0\n",
      "[Deepseek] Processing question 5/50: 4.0\n",
      "Warning: No Deepseek data for question 4.0\n",
      "[Deepseek] Processing question 6/50: 5.0\n",
      "[Deepseek] Processing question 7/50: 6.0\n",
      "Warning: No Deepseek data for question 6.0\n",
      "[Deepseek] Processing question 8/50: 6.0\n",
      "Warning: No Claude data for question 6.0\n",
      "[Deepseek] Processing question 9/50: 7.0\n",
      "Warning: No Deepseek data for question 7.0\n",
      "[Deepseek] Processing question 10/50: 7.0\n",
      "Warning: No Claude data for question 7.0\n",
      "[Deepseek] Processing question 11/50: 8.0\n",
      "[Deepseek] Processing question 12/50: 9.0\n",
      "[Deepseek] Processing question 13/50: 10.0\n",
      "[Deepseek] Processing question 14/50: 11.0\n",
      "[Deepseek] Processing question 15/50: 12.0\n",
      "[Deepseek] Processing question 16/50: 13.0\n",
      "[Deepseek] Processing question 17/50: 14.0\n",
      "[Deepseek] Processing question 18/50: 15.0\n",
      "Warning: No Deepseek data for question 15.0\n",
      "[Deepseek] Processing question 19/50: 15.0\n",
      "Warning: No Claude data for question 15.0\n",
      "[Deepseek] Processing question 20/50: 16.0\n",
      "[Deepseek] Processing question 21/50: 17.0\n",
      "[Deepseek] Processing question 22/50: 18.0\n",
      "[Deepseek] Processing question 23/50: 19.0\n",
      "[Deepseek] Processing question 24/50: 20.0\n",
      "[Deepseek] Processing question 25/50: 21.0\n",
      "[Deepseek] Processing question 26/50: 22.0\n",
      "[Deepseek] Processing question 27/50: 23.0\n",
      "[Deepseek] Processing question 28/50: 24.0\n",
      "[Deepseek] Processing question 29/50: 25.0\n",
      "[Deepseek] Processing question 30/50: 26.0\n",
      "[Deepseek] Processing question 31/50: 27.0\n",
      "[Deepseek] Processing question 32/50: 28.0\n",
      "[Deepseek] Processing question 33/50: 29.0\n",
      "[Deepseek] Processing question 34/50: 30.0\n",
      "Warning: No Claude data for question 30.0\n",
      "[Deepseek] Processing question 35/50: 30.0\n",
      "Warning: No Deepseek data for question 30.0\n",
      "[Deepseek] Processing question 36/50: 31.0\n",
      "[Deepseek] Processing question 37/50: 32.0\n",
      "[Deepseek] Processing question 38/50: 33.0\n",
      "[Deepseek] Processing question 39/50: 34.0\n",
      "[Deepseek] Processing question 40/50: 35.0\n",
      "[Deepseek] Processing question 41/50: 36.0\n",
      "[Deepseek] Processing question 42/50: 37.0\n",
      "[Deepseek] Processing question 43/50: 38.0\n",
      "[Deepseek] Processing question 44/50: 39.0\n",
      "[Deepseek] Processing question 45/50: 40.0\n",
      "[Deepseek] Processing question 46/50: 41.0\n",
      "[Deepseek] Processing question 47/50: 42.0\n",
      "[Deepseek] Processing question 48/50: 43.0\n",
      "[Deepseek] Processing question 49/50: 44.0\n",
      "[Deepseek] Processing question 50/50: 45.0\n"
     ]
    }
   ],
   "source": [
    "# Function to run the full evaluation and display results\n",
    "print(\"Starting medical diagnosis evaluation...\")\n",
    "results_df, overall_metrics = process_medical_diagnoses()\n",
    "\n",
    "if results_df is not None:\n",
    "    results_df.to_csv('deepseek_nonrag_results.csv', index=False)\n",
    "else:\n",
    "    print(\"Error: Evaluation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e02445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Similarity_Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>matched</th>\n",
       "      <th>total_ground_truth</th>\n",
       "      <th>total_predictions</th>\n",
       "      <th>answer_cosine_similarity</th>\n",
       "      <th>full_answer_cosine_similarity</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "      <th>bert_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.288060</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.325926</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.155556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.199705</td>\n",
       "      <td>0.197696</td>\n",
       "      <td>0.116504</td>\n",
       "      <td>0.222519</td>\n",
       "      <td>0.770126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.133926</td>\n",
       "      <td>0.236167</td>\n",
       "      <td>0.476731</td>\n",
       "      <td>0.279690</td>\n",
       "      <td>0.326745</td>\n",
       "      <td>0.476731</td>\n",
       "      <td>0.737180</td>\n",
       "      <td>0.317821</td>\n",
       "      <td>0.174779</td>\n",
       "      <td>0.185402</td>\n",
       "      <td>0.098355</td>\n",
       "      <td>0.225482</td>\n",
       "      <td>0.282450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.043315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.801265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.293931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170776</td>\n",
       "      <td>0.268581</td>\n",
       "      <td>0.095273</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.872811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.414521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.260556</td>\n",
       "      <td>0.353087</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.912232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.919631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.732105</td>\n",
       "      <td>0.464891</td>\n",
       "      <td>0.380341</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.966148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              No  Similarity_Score  precision     recall   f1_score  \\\n",
       "count  45.000000         45.000000  45.000000  45.000000  45.000000   \n",
       "mean   23.000000          0.288060   0.666667   0.325926   0.425926   \n",
       "std    13.133926          0.236167   0.476731   0.279690   0.326745   \n",
       "min     1.000000          0.000000   0.000000   0.000000   0.000000   \n",
       "25%    12.000000          0.043315   0.000000   0.000000   0.000000   \n",
       "50%    23.000000          0.293931   1.000000   0.333333   0.500000   \n",
       "75%    34.000000          0.414521   1.000000   0.500000   0.666667   \n",
       "max    45.000000          0.919631   1.000000   1.000000   1.000000   \n",
       "\n",
       "         matched  total_ground_truth  total_predictions  \\\n",
       "count  45.000000           45.000000          45.000000   \n",
       "mean    0.666667            2.155556           0.888889   \n",
       "std     0.476731            0.737180           0.317821   \n",
       "min     0.000000            1.000000           0.000000   \n",
       "25%     0.000000            2.000000           1.000000   \n",
       "50%     1.000000            2.000000           1.000000   \n",
       "75%     1.000000            3.000000           1.000000   \n",
       "max     1.000000            4.000000           1.000000   \n",
       "\n",
       "       answer_cosine_similarity  full_answer_cosine_similarity  bleu_score  \\\n",
       "count                 45.000000                      45.000000   45.000000   \n",
       "mean                   0.199705                       0.197696    0.116504   \n",
       "std                    0.174779                       0.185402    0.098355   \n",
       "min                    0.000000                       0.000000    0.000000   \n",
       "25%                    0.101631                       0.000000    0.055584   \n",
       "50%                    0.170776                       0.268581    0.095273   \n",
       "75%                    0.260556                       0.353087    0.158788   \n",
       "max                    0.732105                       0.464891    0.380341   \n",
       "\n",
       "       meteor_score  bert_score  \n",
       "count     45.000000   45.000000  \n",
       "mean       0.222519    0.770126  \n",
       "std        0.225482    0.282450  \n",
       "min        0.000000    0.000000  \n",
       "25%        0.066667    0.801265  \n",
       "50%        0.166667    0.872811  \n",
       "75%        0.250000    0.912232  \n",
       "max        0.892256    0.966148  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
