{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2024b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ee7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = pd.read_csv('result/bert_bleurt_result.csv')\n",
    "llm_df = pd.read_csv('result/LLM_judge_result.csv')\n",
    "sas_df = pd.read_csv('result/sas_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32276808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45 entries, 0 to 44\n",
      "Data columns (total 42 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Unnamed: 0                       45 non-null     int64  \n",
      " 1   No                               45 non-null     float64\n",
      " 2   Question                         45 non-null     object \n",
      " 3   Claude_P                         45 non-null     float64\n",
      " 4   Claude_R                         45 non-null     float64\n",
      " 5   Claude_F1                        45 non-null     float64\n",
      " 6   Claude_BLEURT                    45 non-null     float64\n",
      " 7   Qwen_P                           45 non-null     float64\n",
      " 8   Qwen_R                           45 non-null     float64\n",
      " 9   Qwen_F1                          45 non-null     float64\n",
      " 10  Qwen_BLEURT                      45 non-null     float64\n",
      " 11  GPT_P                            45 non-null     float64\n",
      " 12  GPT_R                            45 non-null     float64\n",
      " 13  GPT_F1                           45 non-null     float64\n",
      " 14  GPT_BLEURT                       45 non-null     float64\n",
      " 15  Deepseek RAG_P                   45 non-null     float64\n",
      " 16  Deepseek RAG_R                   45 non-null     float64\n",
      " 17  Deepseek RAG_F1                  45 non-null     float64\n",
      " 18  Deepseek RAG_BLEURT              45 non-null     float64\n",
      " 19  Deepseek non RAG_P               45 non-null     float64\n",
      " 20  Deepseek non RAG_R               45 non-null     float64\n",
      " 21  Deepseek non RAG_F1              45 non-null     float64\n",
      " 22  Deepseek non RAG_BLEURT          45 non-null     float64\n",
      " 23  Dr Answer                        45 non-null     object \n",
      " 24  Claude Answer                    45 non-null     object \n",
      " 25  Qwen Answer                      45 non-null     object \n",
      " 26  GPT Answer                       45 non-null     object \n",
      " 27  Deepseek RAG Answer              45 non-null     object \n",
      " 28  Deepseek non RAG Answer          45 non-null     object \n",
      " 29  Claude Full Answer               45 non-null     object \n",
      " 30  Qwen Full Answer                 45 non-null     object \n",
      " 31  GPT Full Answer                  45 non-null     object \n",
      " 32  Deepseek RAG Full Answer         45 non-null     object \n",
      " 33  Deepseek non RAG Full Answer     45 non-null     object \n",
      " 34  LLM_JUDGE_Qwen_score             45 non-null     float64\n",
      " 35  LLM_JUDGE_GPT_score              45 non-null     float64\n",
      " 36  LLM_JUDGE_Deepseek_RAG_score     45 non-null     float64\n",
      " 37  LLM_JUDGE_Deepseek_nonRAG_score  45 non-null     float64\n",
      " 38  SAS_Qwen_cross_score             45 non-null     float64\n",
      " 39  SAS_GPT_cross_score              45 non-null     float64\n",
      " 40  SAS_Deepseek_RAG_cross_score     45 non-null     float64\n",
      " 41  SAS_Deepseek_nonRAG_cross_score  45 non-null     float64\n",
      "dtypes: float64(29), int64(1), object(12)\n",
      "memory usage: 14.9+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df = (\n",
    "    bert_df\n",
    "    .merge(llm_df, on='No', how='left', suffixes=('', '_dup1'))\n",
    "    .merge(sas_df, on='No', how='left', suffixes=('', '_dup2'))\n",
    ")\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('_dup')]\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('_reasoning')]\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('_category')]\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a903342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapiro_wilk(data):\n",
    "    stat, p = stats.shapiro(data)\n",
    "    # print(f\"Shapiro-Wilk Statistic = {stat:.4f}\")\n",
    "    # print(f\"p-value = {p:.4f}\")\n",
    "    return p\n",
    "\n",
    "data_deepseek_rag = merged_df['Deepseek RAG_P']\n",
    "data_deepseek_non_rag = merged_df['Deepseek non RAG_P']\n",
    "count_p_val(data_rag)\n",
    "count_p_val(data_non_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82756b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p_two_tailed = stats.ttest_rel(data_rag, data_non_rag)\n",
    "if t_stat > 0:\n",
    "    p_one_tailed = p_two_tailed / 2\n",
    "else:\n",
    "    p_one_tailed = 1 - (p_two_tailed / 2)\n",
    "\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"One-tailed p-value = {p_one_tailed:.4f}\")\n",
    "print(f\"Two-tailed p-value = {p_two_tailed:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be383f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gpt = eval_df['GPT_P']\n",
    "data_qwen = eval_df['Qwen_P']\n",
    "count_p_val(data_gpt)\n",
    "count_p_val(data_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_df = pd.DataFrame({\n",
    "    'GPT': data_gpt,\n",
    "    'Deepseek': data_rag,\n",
    "    'Qwen': data_qwen\n",
    "}).melt(var_name='model', value_name='score')\n",
    "\n",
    "# Each observation corresponds to same test item\n",
    "anova_df['subject'] = np.tile(np.arange(len(data_gpt)), 3)\n",
    "\n",
    "anova = pg.rm_anova(dv='score', within='model', subject='subject', data=anova_df, detailed=True)\n",
    "print(anova)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
