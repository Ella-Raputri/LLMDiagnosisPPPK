{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4690b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Download required NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2fcbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+%', '', text)  # Remove percentages\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ddd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two texts\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if pd.isna(text1) or pd.isna(text2) or text1 == \"\" or text2 == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate BLEU score between two texts\n",
    "def calculate_bleu_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate_text.lower())\n",
    "    \n",
    "    # Apply smoothing function for short texts\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    try:\n",
    "        # Calculate BLEU score with different n-gram weights\n",
    "        bleu1 = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "        bleu2 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "        bleu3 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "        bleu4 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "        \n",
    "        # Average BLEU scores\n",
    "        avg_bleu = (bleu1 + bleu2 + bleu3 + bleu4) / 4\n",
    "        return avg_bleu\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate METEOR score between two texts\n",
    "def calculate_meteor_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate_text.lower())\n",
    "    \n",
    "    try:\n",
    "        return meteor_score([reference_tokens], candidate_tokens)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate BERT score\n",
    "def calculate_bert_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode texts\n",
    "    inputs1 = tokenizer(reference_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs2 = tokenizer(candidate_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs1 = {k: v.to(device) for k, v in inputs1.items()}\n",
    "    inputs2 = {k: v.to(device) for k, v in inputs2.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "    \n",
    "    # Use CLS token embeddings for sentence representation\n",
    "    embeddings1 = outputs1.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    embeddings2 = outputs2.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Calculate cosine similarity between embeddings\n",
    "    similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3b25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved function to standardize disease names\n",
    "def standardize_disease_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    \n",
    "    name = str(name).lower().strip()\n",
    "    # Remove percentages\n",
    "    name = re.sub(r'\\d+%', '', name).strip()\n",
    "    \n",
    "    # Common variations of disease names to standardize - EXPANDED\n",
    "    mapping = {\n",
    "        # Demam (Fever) related\n",
    "        'dbd': 'demam berdarah dengue',\n",
    "        'dengue fever': 'demam berdarah dengue',\n",
    "        'demam dengue': 'demam berdarah dengue',\n",
    "        'dengue': 'demam berdarah dengue',\n",
    "        'demam berdarah': 'demam berdarah dengue',\n",
    "        'demam biasa': 'common fever',\n",
    "        'common fever': 'common fever',\n",
    "        'demam umum': 'common fever',\n",
    "        \n",
    "        # Gastro related\n",
    "        'gastroenteritis akut': 'gastroenteritis',\n",
    "        'gastroenteritis (ge) akut': 'gastroenteritis',\n",
    "        'ge akut': 'gastroenteritis',\n",
    "        'gastroenteritis': 'gastroenteritis',\n",
    "        'diare akut': 'gastroenteritis',\n",
    "        \n",
    "        # Respiratory related\n",
    "        'infeksi saluran pernapasan atas': 'ispa',\n",
    "        'ispa': 'ispa',\n",
    "        'infeksi saluran napas atas': 'ispa',\n",
    "        'infeksi saluran pernafasan atas': 'ispa',\n",
    "        'infeksi saluran pernapasan': 'ispa',\n",
    "        \n",
    "        # GERD related\n",
    "        'reflux gastroesofagus': 'gerd',\n",
    "        'reflux asam lambung': 'gerd',\n",
    "        'refleks asam lambung': 'gerd',\n",
    "        'gastroesophageal reflux disease': 'gerd',\n",
    "        'refleks gastroesofagus': 'gerd',\n",
    "        'gerd': 'gerd',\n",
    "        \n",
    "        # Gastritis related\n",
    "        'maag': 'gastritis',\n",
    "        'gastritis akut': 'gastritis',\n",
    "        'penyakit maag': 'gastritis',\n",
    "        'penyakit maag akut': 'gastritis',\n",
    "        'gastritis': 'gastritis',\n",
    "        \n",
    "        # Heart related\n",
    "        'infark miokard akut': 'serangan jantung',\n",
    "        'serangan jantung': 'serangan jantung',\n",
    "        'angina pektoris': 'angina',\n",
    "        'angina': 'angina',\n",
    "        \n",
    "        # Asthma related\n",
    "        'asma bronkial': 'asma',\n",
    "        'asma exacerbation': 'asma',\n",
    "        'asma': 'asma',\n",
    "        'pemburukan asma': 'asma',\n",
    "        \n",
    "        # Bronchitis related\n",
    "        'bronkitis akut': 'bronkitis',\n",
    "        'bronkitis': 'bronkitis',\n",
    "        \n",
    "        # Wound related\n",
    "        'vulnus laceratum': 'luka robek',\n",
    "        'luka robek': 'luka robek',\n",
    "        'luka terbuka': 'luka robek',\n",
    "        'laceration': 'luka robek',\n",
    "        'vulnus excoriatum': 'luka lecet',\n",
    "        'luka lecet': 'luka lecet',\n",
    "        \n",
    "        # Head injury related\n",
    "        'cedera kepala ringan': 'ckr',\n",
    "        'kepala cedera ringan': 'ckr',\n",
    "        'ckr': 'ckr',\n",
    "        \n",
    "        # Dyspepsia related\n",
    "        'dispepsia': 'dispepsia',\n",
    "        'dispepsia fungsional': 'dispepsia',\n",
    "        \n",
    "        # Appendicitis related\n",
    "        'appendisitis akut': 'appendisitis',\n",
    "        'appendisitis': 'appendisitis',\n",
    "        'apendisitis': 'appendisitis',\n",
    "        \n",
    "        # UTI related\n",
    "        'infeksi saluran kemih': 'isk',\n",
    "        'isk': 'isk',\n",
    "        'infeksi saluran kemih akut': 'isk',\n",
    "        \n",
    "        # Additional mappings\n",
    "        'intoleransi laktosa': 'intoleransi laktosa',\n",
    "        'pneumonia': 'pneumonia',\n",
    "        'hipertensi': 'hipertensi',\n",
    "        'hipertensi akut': 'hipertensi',\n",
    "        'hipertensi stage 1': 'hipertensi',\n",
    "        'luka bakar': 'luka bakar',\n",
    "        'burn injury': 'luka bakar',\n",
    "        'tonsilitis': 'tonsilitis',\n",
    "        'faringitis': 'faringitis',\n",
    "        'vertigo': 'vertigo',\n",
    "        'benign paroxysmal positional vertigo': 'vertigo',\n",
    "        'bppv': 'vertigo',\n",
    "        'influenza': 'influenza',\n",
    "        'kolik renal': 'kolik renal',\n",
    "        'batu ginjal': 'batu ginjal',\n",
    "        'peritonitis': 'peritonitis'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping if available - using partial matching for more flexibility\n",
    "    for key, value in mapping.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    \n",
    "    # If no mapping found, return the cleaned name\n",
    "    return name\n",
    "\n",
    "# Function to extract percentage from diagnosis text\n",
    "def extract_percentage(text):\n",
    "    match = re.search(r'(\\d+)%', text)\n",
    "    return float(match.group(1))/100 if match else 0.5  # Default to 50% if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd8ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse diagnoses from raw answer text\n",
    "def parse_diagnoses(answer_text):\n",
    "    if pd.isna(answer_text):\n",
    "        return []\n",
    "    \n",
    "    # Split by semicolon and process each diagnosis\n",
    "    diagnoses = []\n",
    "    percentages = []\n",
    "    \n",
    "    for item in answer_text.split(';'):\n",
    "        item = item.strip()\n",
    "        if not item:\n",
    "            continue\n",
    "            \n",
    "        # Extract diagnosis name and percentage \n",
    "        match = re.search(r'(.*?)(?:\\s+(\\d+)%)?$', item)\n",
    "        if match and match.group(1).strip():\n",
    "            diagnoses.append(standardize_disease_name(match.group(1).strip()))\n",
    "            percentages.append(extract_percentage(item))\n",
    "    \n",
    "    # Create a list of tuples (diagnosis, percentage)\n",
    "    return list(zip(diagnoses, percentages))\n",
    "\n",
    "# Main processing function to analyze and evaluate diagnoses\n",
    "def process_medical_diagnoses():\n",
    "    # Load Excel file with both models' outputs\n",
    "    xlsx_path = '30 sample penyakit - hasil prompt LLM.xlsx'\n",
    "    \n",
    "    try:\n",
    "        # Load raw data from both sheets\n",
    "        ground_truth_raw = pd.read_excel(xlsx_path, sheet_name='Ground Truth')\n",
    "        qwen_raw = pd.read_excel(xlsx_path, sheet_name='Qwen 2.5 72B')\n",
    "        \n",
    "        # Fill forward the question IDs and questions\n",
    "        ground_truth_raw['No'] = ground_truth_raw['No'].ffill()\n",
    "        ground_truth_raw['Question'] = ground_truth_raw['Question'].ffill()\n",
    "        qwen_raw['No'] = qwen_raw['No'].ffill()\n",
    "        qwen_raw['Question'] = qwen_raw['Question'].ffill()\n",
    "        \n",
    "        # Group by question to organize multiple answers per question\n",
    "        ground_truth_grouped = ground_truth_raw.groupby(['No', 'Question'])\n",
    "        qwen_grouped = qwen_raw.groupby(['No', 'Question'])\n",
    "        \n",
    "        # Create the unified question list - ensure we have all questions\n",
    "        all_questions = list(set(ground_truth_grouped.groups.keys()) | set(qwen_grouped.groups.keys()))\n",
    "        all_questions.sort(key=lambda x: float(x[0]) if x[0] is not None and not pd.isna(x[0]) else float('inf'))\n",
    "        \n",
    "        # Results container\n",
    "        results = []\n",
    "        \n",
    "        # Process each question\n",
    "        for idx, (no, question) in enumerate(all_questions):\n",
    "            try:\n",
    "                print(f\"Processing question {idx+1}/{len(all_questions)}: {no}\")\n",
    "                \n",
    "                # Get Ground Truth answers (ground truth)\n",
    "                if (no, question) in ground_truth_grouped.groups:\n",
    "                    ground_truth_rows = ground_truth_grouped.get_group((no, question))\n",
    "                    ground_truth_answers = ground_truth_rows['Answer'].tolist()\n",
    "                    ground_truth_diagnoses = []\n",
    "                    \n",
    "                    # Parse all ground truth answers to get diagnoses with percentages\n",
    "                    for ans in ground_truth_answers:\n",
    "                        if not pd.isna(ans):\n",
    "                            ground_truth_diagnoses.extend(parse_diagnoses(ans))\n",
    "                    \n",
    "                    # Since Full Answer column is removed from Ground Truth, we'll use Answer column instead\n",
    "                    ground_truth_full_text = ' '.join([text for text in ground_truth_answers if not pd.isna(text)])\n",
    "                else:\n",
    "                    # Skip if no ground truth available\n",
    "                    print(f\"Warning: No Ground Truth data for question {no}\")\n",
    "                    continue\n",
    "\n",
    "                # Get Qwen candidate answers\n",
    "                if (no, question) in qwen_grouped.groups:\n",
    "                    qwen_rows = qwen_grouped.get_group((no, question))\n",
    "                    qwen_answers = [ans for ans in qwen_rows['Answer'].tolist() if not pd.isna(ans)]\n",
    "                    \n",
    "                    # Select the best matching Qwen answer\n",
    "                    best_qwen, similarity_score = select_best_qwen_answer(qwen_answers, ground_truth_diagnoses)\n",
    "                    \n",
    "                    # Parse the best Qwen answer\n",
    "                    qwen_diagnoses = parse_diagnoses(best_qwen)\n",
    "                    \n",
    "                    # Get Qwen full text matching the best answer\n",
    "                    qwen_explanation_rows = qwen_rows[qwen_rows['Answer'] == best_qwen]\n",
    "                    qwen_explanations = [text for text in qwen_explanation_rows['Full Answer'].tolist() if not pd.isna(text)]\n",
    "                    qwen_full_text = ' '.join(qwen_explanations) if qwen_explanations else \"\"\n",
    "                else:\n",
    "                    print(f\"Warning: No Qwen data for question {no}\")\n",
    "                    best_qwen = \"\"\n",
    "                    similarity_score = 0.0\n",
    "                    qwen_diagnoses = []\n",
    "                    qwen_full_text = \"\"\n",
    "                \n",
    "                # Calculate diagnosis match metrics\n",
    "                diagnosis_metrics = calculate_diagnosis_match_metrics(ground_truth_diagnoses, qwen_diagnoses)\n",
    "                \n",
    "                # Calculate NLP similarity metrics\n",
    "                # For answer (diagnosis names only)\n",
    "                ground_truth_answer_text = '; '.join([f\"{d} {int(p*100)}%\" for d, p in ground_truth_diagnoses])\n",
    "                \n",
    "                nlp_metrics = {\n",
    "                    'answer_cosine_similarity': calculate_cosine_similarity(\n",
    "                        preprocess_text(ground_truth_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    ),\n",
    "                    'bleu_score': calculate_bleu_score(\n",
    "                        preprocess_text(ground_truth_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    ),\n",
    "                    'meteor_score': calculate_meteor_score(\n",
    "                        preprocess_text(ground_truth_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    ),\n",
    "                    'bert_score': calculate_bert_score(\n",
    "                        preprocess_text(ground_truth_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    )\n",
    "                }\n",
    "                \n",
    "                # Combined all metrics into one result entry\n",
    "                result = {\n",
    "                    'No': no,\n",
    "                    'Question': question,\n",
    "                    'Ground_Truth_Diagnoses': ground_truth_answer_text,\n",
    "                    'Selected_Qwen_Answer': best_qwen,\n",
    "                    'Similarity_Score': similarity_score,\n",
    "                    **diagnosis_metrics,\n",
    "                    **nlp_metrics\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing question {no}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Create final results dataframe\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        overall_metrics = {\n",
    "            'total_questions': len(results),\n",
    "            'average_precision': results_df['precision'].mean(),\n",
    "            'average_recall': results_df['recall'].mean(),\n",
    "            'average_f1_score': results_df['f1_score'].mean(),\n",
    "            'average_weighted_precision': results_df['weighted_precision'].mean(),\n",
    "            'average_weighted_recall': results_df['weighted_recall'].mean(),\n",
    "            'average_weighted_f1_score': results_df['weighted_f1_score'].mean(),\n",
    "            'average_answer_cosine_similarity': results_df['answer_cosine_similarity'].mean(),\n",
    "            'average_bleu_score': results_df['bleu_score'].mean(),\n",
    "            'average_meteor_score': results_df['meteor_score'].mean(),\n",
    "            'average_bert_score': results_df['bert_score'].mean()\n",
    "        }\n",
    "        \n",
    "        return results_df, overall_metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel file: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41068d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the best matching Qwen answer for a given Ground Truth answer\n",
    "def select_best_qwen_answer(qwen_answers, ground_truth_diagnoses):\n",
    "    \"\"\"\n",
    "    Selects the best matching Qwen answer based on overlap with Ground Truth diagnoses\n",
    "    \n",
    "    Args:\n",
    "        qwen_answers: List of Qwen answer strings\n",
    "        ground_truth_diagnoses: List of (diagnosis, score) tuples from Ground Truth\n",
    "    \n",
    "    Returns:\n",
    "        best_answer: The Qwen answer with highest similarity score\n",
    "    \"\"\"\n",
    "    if not qwen_answers or not ground_truth_diagnoses:\n",
    "        return \"\"\n",
    "    \n",
    "    best_score = -1\n",
    "    best_answer = \"\"\n",
    "    \n",
    "    ground_truth_diseases = [d[0] for d in ground_truth_diagnoses]\n",
    "    \n",
    "    for qwen_ans in qwen_answers:\n",
    "        # Parse the current Qwen answer\n",
    "        qwen_parsed = parse_diagnoses(qwen_ans)\n",
    "        qwen_diseases = [d[0] for d in qwen_parsed]\n",
    "        \n",
    "        # Calculate overlap score - how many diseases match\n",
    "        match_count = sum(1 for d in qwen_diseases if d in ground_truth_diseases)\n",
    "        match_score = match_count / max(len(ground_truth_diseases), len(qwen_diseases), 1)\n",
    "        \n",
    "        # Also consider text similarity as fallback\n",
    "        text_sim = calculate_cosine_similarity(\n",
    "            preprocess_text(qwen_ans), \n",
    "            preprocess_text('; '.join([d for d, _ in ground_truth_diagnoses]))\n",
    "        )\n",
    "        \n",
    "        # Combined score favoring matching diseases but also considering text similarity\n",
    "        combined_score = (match_score * 0.7) + (text_sim * 0.3)\n",
    "        \n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_answer = qwen_ans\n",
    "            \n",
    "    return best_answer, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4daec43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diagnosis_match_metrics(ground_truth_diagnoses, qwen_diagnoses):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for disease matching with weighted scores\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_diagnoses: List of (disease, score) tuples from Ground Truth\n",
    "        qwen_diagnoses: List of (disease, score) tuples from Qwen\n",
    "    \n",
    "    Returns:\n",
    "        dict with precision, recall, f1_score, etc.\n",
    "    \"\"\"\n",
    "    if not ground_truth_diagnoses or not qwen_diagnoses:\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'weighted_precision': 0.0,\n",
    "            'weighted_recall': 0.0,\n",
    "            'weighted_f1_score': 0.0,\n",
    "            'matched': 0,\n",
    "            'total_ground_truth': len(ground_truth_diagnoses),\n",
    "            'total_predictions': len(qwen_diagnoses)\n",
    "        }\n",
    "    \n",
    "    # Extract disease names and their weights\n",
    "    ground_truth_diseases = [d[0] for d in ground_truth_diagnoses]\n",
    "    ground_truth_weights = [d[1] for d in ground_truth_diagnoses]\n",
    "    qwen_diseases = [d[0] for d in qwen_diagnoses]\n",
    "    qwen_weights = [d[1] for d in qwen_diagnoses]\n",
    "    \n",
    "    # Calculate regular metrics\n",
    "    matches = sum(1 for d in qwen_diseases if d in ground_truth_diseases)\n",
    "    precision = matches / len(qwen_diseases) if qwen_diseases else 0\n",
    "    recall = matches / len(ground_truth_diseases) if ground_truth_diseases else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate weighted metrics\n",
    "    weighted_matches = 0\n",
    "    total_ground_truth_weight = sum(ground_truth_weights)\n",
    "    total_qwen_weight = sum(qwen_weights)\n",
    "    \n",
    "    # For each Qwen diagnosis, find matching ground truth and use the higher weight\n",
    "    for i, qwen_disease in enumerate(qwen_diseases):\n",
    "        if qwen_disease in ground_truth_diseases:\n",
    "            gt_idx = ground_truth_diseases.index(qwen_disease)\n",
    "            weighted_matches += max(qwen_weights[i], ground_truth_weights[gt_idx])\n",
    "    \n",
    "    weighted_precision = weighted_matches / total_qwen_weight if total_qwen_weight > 0 else 0\n",
    "    weighted_recall = weighted_matches / total_ground_truth_weight if total_ground_truth_weight > 0 else 0\n",
    "    weighted_f1 = 2 * weighted_precision * weighted_recall / (weighted_precision + weighted_recall) if (weighted_precision + weighted_recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'weighted_precision': weighted_precision,\n",
    "        'weighted_recall': weighted_recall,\n",
    "        'weighted_f1_score': weighted_f1,\n",
    "        'matched': matches,\n",
    "        'total_ground_truth': len(ground_truth_diseases),\n",
    "        'total_predictions': len(qwen_diseases)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86ee91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting medical diagnosis evaluation...\n",
      "Processing question 1/46: 1.0\n",
      "Processing question 2/46: 2.0\n",
      "Processing question 3/46: 3.0\n",
      "Processing question 4/46: 4.0\n",
      "Processing question 5/46: 5.0\n",
      "Processing question 6/46: 6.0\n",
      "Processing question 7/46: 7.0\n",
      "Warning: No Ground Truth data for question 7.0\n",
      "Processing question 8/46: 7.0\n",
      "Warning: No Qwen data for question 7.0\n",
      "Processing question 9/46: 8.0\n",
      "Processing question 10/46: 9.0\n",
      "Processing question 11/46: 10.0\n",
      "Processing question 12/46: 11.0\n",
      "Processing question 13/46: 12.0\n",
      "Processing question 14/46: 13.0\n",
      "Processing question 15/46: 14.0\n",
      "Processing question 16/46: 15.0\n",
      "Processing question 17/46: 16.0\n",
      "Processing question 18/46: 17.0\n",
      "Processing question 19/46: 18.0\n",
      "Processing question 20/46: 19.0\n",
      "Processing question 21/46: 20.0\n",
      "Processing question 22/46: 21.0\n",
      "Processing question 23/46: 22.0\n",
      "Processing question 24/46: 23.0\n",
      "Processing question 25/46: 24.0\n",
      "Processing question 26/46: 25.0\n",
      "Processing question 27/46: 26.0\n",
      "Processing question 28/46: 27.0\n",
      "Processing question 29/46: 28.0\n",
      "Processing question 30/46: 29.0\n",
      "Processing question 31/46: 30.0\n",
      "Processing question 32/46: 31.0\n",
      "Processing question 33/46: 32.0\n",
      "Processing question 34/46: 33.0\n",
      "Processing question 35/46: 34.0\n",
      "Processing question 36/46: 35.0\n",
      "Processing question 37/46: 36.0\n",
      "Processing question 38/46: 37.0\n",
      "Processing question 39/46: 38.0\n",
      "Processing question 40/46: 39.0\n",
      "Processing question 41/46: 40.0\n",
      "Processing question 42/46: 41.0\n",
      "Processing question 43/46: 42.0\n",
      "Processing question 44/46: 43.0\n",
      "Processing question 45/46: 44.0\n",
      "Processing question 46/46: 45.0\n",
      "\n",
      "=== OVERALL EVALUATION METRICS ===\n",
      "Total Questions: 45.0000\n",
      "Average Precision: 0.7778\n",
      "Average Recall: 0.3926\n",
      "Average F1 Score: 0.5074\n",
      "Average Weighted Precision: 1.1556\n",
      "Average Weighted Recall: 0.4079\n",
      "Average Weighted F1 Score: 0.5915\n",
      "Average Answer Cosine Similarity: 0.3470\n",
      "Average Bleu Score: 0.1485\n",
      "Average Meteor Score: 0.2462\n",
      "Average Bert Score: 0.8331\n",
      "\n",
      "=== TOP 5 QUESTIONS WITH HIGHEST WEIGHTED F1 SCORES ===\n",
      "Question 26.0:\n",
      "  Regular F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Weighted F1 = 1.3333, Precision = 2.0000, Recall = 1.0000\n",
      "  Ground Truth: luka robek 100%\n",
      "  Qwen: Luka Robek (Laceration)\n",
      "\n",
      "Question 29.0:\n",
      "  Regular F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Weighted F1 = 1.3333, Precision = 2.0000, Recall = 1.0000\n",
      "  Ground Truth: luka lecet 100%\n",
      "  Qwen: Luka Lecet\n",
      "\n",
      "Question 20.0:\n",
      "  Regular F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Weighted F1 = 1.2308, Precision = 1.6000, Recall = 1.0000\n",
      "  Ground Truth: ispa 80%\n",
      "  Qwen: Infeksi Saluran Pernapasan Atas (ISPA)\n",
      "\n",
      "Question 3.0:\n",
      "  Regular F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Weighted F1 = 1.2308, Precision = 1.6000, Recall = 1.0000\n",
      "  Ground Truth: demam berdarah dengue 80%\n",
      "  Qwen: Demam Dengue\n",
      "\n",
      "Question 31.0:\n",
      "  Regular F1 = 0.6667, Precision = 1.0000, Recall = 0.5000\n",
      "  Weighted F1 = 0.9333, Precision = 1.4000, Recall = 0.7000\n",
      "  Ground Truth: common fever 70%; demam berdarah dengue 30%\n",
      "  Qwen: Demam Biasa\n",
      "\n",
      "\n",
      "=== QUESTIONS WITH LOWEST WEIGHTED F1 SCORES ===\n",
      "Question 1.0:\n",
      "  Regular F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Weighted F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Ground Truth: demam tifoid 70%; demam berdarah dengue 30%\n",
      "  Qwen: Demam Biasa\n",
      "\n",
      "Question 4.0:\n",
      "  Regular F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Weighted F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Ground Truth: infeksi bakteri/virus (sistemik) 50%; infeksi jaringan lunak (selulitis) 70%\n",
      "  Qwen: Infeksi Lokal\n",
      "\n",
      "Question 7.0:\n",
      "  Regular F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Weighted F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Ground Truth: bronkitis 100%\n",
      "  Qwen: \n",
      "\n",
      "Question 12.0:\n",
      "  Regular F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Weighted F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Ground Truth: batu ginjal 70%; kolik renal 80%; ganguan saluran kemih 100%; ureterolitiasis 70%\n",
      "  Qwen: Infeksi Saluran Kemih (ISK) Akut\n",
      "\n",
      "Question 28.0:\n",
      "  Regular F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Weighted F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Ground Truth: ckr 90%; luka robek 100%; gangguan keseimbangan neurologis lain 70%\n",
      "  Qwen: Benign Paroxysmal Positional Vertigo (BPPV)\n",
      "\n",
      "\n",
      "Results saved to medical_diagnosis_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to run the full evaluation and display results\n",
    "def main():\n",
    "    print(\"Starting medical diagnosis evaluation...\")\n",
    "    results_df, overall_metrics = process_medical_diagnoses()\n",
    "    \n",
    "    if results_df is not None:\n",
    "        # Display summary statistics\n",
    "        print(\"\\n=== OVERALL EVALUATION METRICS ===\")\n",
    "        for metric, value in overall_metrics.items():\n",
    "            print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        \n",
    "        # Display per-question results\n",
    "        print(\"\\n=== TOP 5 QUESTIONS WITH HIGHEST WEIGHTED F1 SCORES ===\")\n",
    "        top_questions = results_df.sort_values('weighted_f1_score', ascending=False).head(5)\n",
    "        for _, row in top_questions.iterrows():\n",
    "            print(f\"Question {row['No']}:\")\n",
    "            print(f\"  Regular F1 = {row['f1_score']:.4f}, Precision = {row['precision']:.4f}, Recall = {row['recall']:.4f}\")\n",
    "            print(f\"  Weighted F1 = {row['weighted_f1_score']:.4f}, Precision = {row['weighted_precision']:.4f}, Recall = {row['weighted_recall']:.4f}\")\n",
    "            print(f\"  Ground Truth: {row['Ground_Truth_Diagnoses']}\")\n",
    "            print(f\"  Qwen: {row['Selected_Qwen_Answer']}\")\n",
    "            print()\n",
    "\n",
    "        # Display questions with lowest scores\n",
    "        print(\"\\n=== QUESTIONS WITH LOWEST WEIGHTED F1 SCORES ===\")\n",
    "        bottom_questions = results_df.sort_values('weighted_f1_score').head(5)\n",
    "        for _, row in bottom_questions.iterrows():\n",
    "            print(f\"Question {row['No']}:\")\n",
    "            print(f\"  Regular F1 = {row['f1_score']:.4f}, Precision = {row['precision']:.4f}, Recall = {row['recall']:.4f}\")\n",
    "            print(f\"  Weighted F1 = {row['weighted_f1_score']:.4f}, Precision = {row['weighted_precision']:.4f}, Recall = {row['weighted_recall']:.4f}\")\n",
    "            print(f\"  Ground Truth: {row['Ground_Truth_Diagnoses']}\")\n",
    "            print(f\"  Qwen: {row['Selected_Qwen_Answer']}\")\n",
    "            print()\n",
    "        \n",
    "        # Save results to CSV \n",
    "        results_df.to_csv('medical_diagnosis_evaluation_results.csv', index=False)\n",
    "        print(\"\\nResults saved to medical_diagnosis_evaluation_results.csv\")\n",
    "    else:\n",
    "        print(\"Error: Evaluation failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ef4f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnalysis of Low Similarity Scores in Medical Diagnosis Evaluation\\n\\nSummary of Scores\\nFrom the evaluation results, we see:\\n- Average Precision: 0.7778 (good)\\n- Average Recall: 0.3926 (low)\\n- Average F1 Score: 0.5074 (moderate)\\n- Text-based metrics:\\n  - Answer Cosine Similarity: 0.3470 (low)\\n  - BLEU Score: 0.1485 (very low)\\n  - METEOR Score: 0.2462 (low)\\n  - BERT Score: 0.8331 (good)\\n\\nWhy Are Some Scores Low?\\n1. Limitations of Text-Based Metrics\\n\\nText similarity metrics like BLEU, METEOR, and cosine similarity have inherent limitations for this task:\\n\\n- BLEU (0.1485) measures n-gram overlap, which is very sensitive to word order and exact matches\\n- METEOR (0.2462) performs better but still depends on exact or stemmed word matches\\n- Cosine similarity (0.3470) works with bag-of-words but loses semantic relationships\\n\\nOnly BERT Score (0.8331) performs well because it\\'s based on contextual embeddings that capture semantic meaning.\\n\\n2. Medical Terminology Variation\\n\\nEven with improved standardization, medical terms can be expressed in many ways:\\n\\n```\\n# Example from Question 28.0:\\nGround Truth: \"ckr 90%; luka robek 100%; gangguan keseimbangan neurologis lain 70%\"\\nQwen: \"Benign Paroxysmal Positional Vertigo (BPPV)\"\\n```\\n\\nBPPV is a specific type of neurological balance disorder, but our mapping doesn\\'t capture this relationship.\\n\\n3. Multilingual Context\\n\\nThe dataset contains a mix of Indonesian and English medical terms:\\n- \"demam berdarah dengue\" vs. \"dengue fever\"\\n- \"luka robek\" vs. \"laceration\"\\n\\nThese variations make text-based matching more challenging.\\n\\n## Suggestions for Improving Scores\\n\\n1. **Enhanced disease mapping**:\\n   - Add more semantic relationships between medical conditions\\n   - Map \"BPPV\" to \"gangguan keseimbangan neurologis\"\\n   - Map \"Infeksi Lokal\" to various infection types\\n\\n2. **Weighted evaluation metrics**:\\n   - Consider the confidence scores when calculating matches\\n   - A diagnosis with 80% confidence should count more than one with 30%\\n\\n3. **Domain-specific evaluation**:\\n   - Develop a medical ontology-based similarity metric\\n   - Use hierarchical relationships (e.g., BPPV is a subtype of balance disorder)\\n\\n4. **Improve dataset alignment**:\\n   - Fix question format inconsistencies\\n   - Ensure both models have answers for all questions\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Overall Performance Analysis\n",
    "\n",
    "Good/High Scores ✅\n",
    "- Average Precision: 0.7778 - It means when Qwen makes a diagnosis prediction, it's correct about 78% of the time\n",
    "- BERT Score: 0.8331 - BERT captures semantic meaning well, showing the model understands medical context even when using different terminology\n",
    "- Weighted Precision: 1.1556 - Very high, indicating the model is particularly accurate for high-confidence diagnoses\n",
    "\n",
    "Poor/Low Scores ❌\n",
    "- Average Recall: 0.3926 - The model only identifies about 39% of the actual diseases present\n",
    "- BLEU Score: 0.1485 - indicating poor word-level overlap\n",
    "- METEOR Score: 0.2462 - Low, showing limited exact word matching\n",
    "- Answer Cosine Similarity: 0.3470 - Low text similarity between responses\n",
    "\n",
    "Moderate Scores ⚠️\n",
    "- F1 Score: 0.5074 - Balanced measure showing overall moderate performance\n",
    "- Weighted F1: 0.5915 - Slightly better when considering confidence levels\n",
    "\n",
    "Why Are Some Scores Low?\n",
    "\n",
    "1. Medical Terminology Variations\n",
    "The model often uses different but medically equivalent terms:\n",
    "```\n",
    "Ground Truth: \"luka robek\" (Indonesian)\n",
    "Qwen: \"Laceration\" (English medical term)\n",
    "```\n",
    "Both are correct, but text similarity metrics can't recognize this equivalence.\n",
    "\n",
    "2. Specificity vs. Generality Mismatch\n",
    "```\n",
    "Ground Truth: \"gangguan keseimbangan neurologis\" (general neurological balance disorder)\n",
    "Qwen: \"Benign Paroxysmal Positional Vertigo (BPPV)\" (specific condition)\n",
    "```\n",
    "BPPV is actually a specific type of balance disorder, so Qwen is more precise, \n",
    "but the evaluation doesn't capture this hierarchical relationship.\n",
    "\n",
    "3. Language Mixing Issues\n",
    "The dataset contains mixed Indonesian-English terminology, \n",
    "making exact matching difficult for traditional NLP metrics.\n",
    "\n",
    "4. Low Recall Problem\n",
    "The model tends to be conservative, making fewer predictions but with higher accuracy. \n",
    "This suggests it might be missing some valid diagnoses to avoid false positives.\n",
    "\n",
    "Why BERT Score is High While Others Are Low\n",
    "\n",
    "BERT Score (0.8331) performs well because it:\n",
    "- Uses contextual embeddings that understand semantic meaning\n",
    "- Can recognize that \"dengue fever\" and \"demam berdarah dengue\" are the same condition\n",
    "- Captures medical relationships between terms\n",
    "\n",
    "Traditional metrics (BLEU, METEOR, Cosine Similarity) fail because they:\n",
    "- Rely on exact word matching\n",
    "- Don't understand medical terminology relationships\n",
    "- Can't handle multilingual medical terms\n",
    "\n",
    "This suggests the model is clinically conservative but semantically competent - \n",
    "a potentially desirable trait in medical AI where false positives can be dangerous.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
