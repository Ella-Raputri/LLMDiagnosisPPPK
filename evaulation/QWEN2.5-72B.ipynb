{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4690b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arija\\OneDrive - Bina Nusantara\\Research Assistant\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Download required NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2fcbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+%', '', text)  # Remove percentages\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ddd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two texts\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if pd.isna(text1) or pd.isna(text2) or text1 == \"\" or text2 == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate BLEU score between two texts\n",
    "def calculate_bleu_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate_text.lower())\n",
    "    \n",
    "    # Apply smoothing function for short texts\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    try:\n",
    "        # Calculate BLEU score with different n-gram weights\n",
    "        bleu1 = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "        bleu2 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "        bleu3 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "        bleu4 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "        \n",
    "        # Average BLEU scores\n",
    "        avg_bleu = (bleu1 + bleu2 + bleu3 + bleu4) / 4\n",
    "        return avg_bleu\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate METEOR score between two texts\n",
    "def calculate_meteor_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate_text.lower())\n",
    "    \n",
    "    try:\n",
    "        return meteor_score([reference_tokens], candidate_tokens)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate BERT score\n",
    "def calculate_bert_score(reference_text, candidate_text):\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text) or reference_text == \"\" or candidate_text == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode texts\n",
    "    inputs1 = tokenizer(reference_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs2 = tokenizer(candidate_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs1 = {k: v.to(device) for k, v in inputs1.items()}\n",
    "    inputs2 = {k: v.to(device) for k, v in inputs2.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "    \n",
    "    # Use CLS token embeddings for sentence representation\n",
    "    embeddings1 = outputs1.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    embeddings2 = outputs2.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Calculate cosine similarity between embeddings\n",
    "    similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3b25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved function to standardize disease names\n",
    "def standardize_disease_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    \n",
    "    name = str(name).lower().strip()\n",
    "    # Remove percentages\n",
    "    name = re.sub(r'\\d+%', '', name).strip()\n",
    "    \n",
    "    # Common variations of disease names to standardize - EXPANDED\n",
    "    mapping = {\n",
    "        # Demam (Fever) related\n",
    "        'dbd': 'demam berdarah dengue',\n",
    "        'dengue fever': 'demam berdarah dengue',\n",
    "        'demam dengue': 'demam berdarah dengue',\n",
    "        'dengue': 'demam berdarah dengue',\n",
    "        'demam berdarah': 'demam berdarah dengue',\n",
    "        'demam biasa': 'common fever',\n",
    "        'common fever': 'common fever',\n",
    "        'demam umum': 'common fever',\n",
    "        \n",
    "        # Gastro related\n",
    "        'gastroenteritis akut': 'gastroenteritis',\n",
    "        'gastroenteritis (ge) akut': 'gastroenteritis',\n",
    "        'ge akut': 'gastroenteritis',\n",
    "        'gastroenteritis': 'gastroenteritis',\n",
    "        'diare akut': 'gastroenteritis',\n",
    "        \n",
    "        # Respiratory related\n",
    "        'infeksi saluran pernapasan atas': 'ispa',\n",
    "        'ispa': 'ispa',\n",
    "        'infeksi saluran napas atas': 'ispa',\n",
    "        'infeksi saluran pernafasan atas': 'ispa',\n",
    "        'infeksi saluran pernapasan': 'ispa',\n",
    "        \n",
    "        # GERD related\n",
    "        'reflux gastroesofagus': 'gerd',\n",
    "        'reflux asam lambung': 'gerd',\n",
    "        'refleks asam lambung': 'gerd',\n",
    "        'gastroesophageal reflux disease': 'gerd',\n",
    "        'refleks gastroesofagus': 'gerd',\n",
    "        'gerd': 'gerd',\n",
    "        \n",
    "        # Gastritis related\n",
    "        'maag': 'gastritis',\n",
    "        'gastritis akut': 'gastritis',\n",
    "        'penyakit maag': 'gastritis',\n",
    "        'penyakit maag akut': 'gastritis',\n",
    "        'gastritis': 'gastritis',\n",
    "        \n",
    "        # Heart related\n",
    "        'infark miokard akut': 'serangan jantung',\n",
    "        'serangan jantung': 'serangan jantung',\n",
    "        'angina pektoris': 'angina',\n",
    "        'angina': 'angina',\n",
    "        \n",
    "        # Asthma related\n",
    "        'asma bronkial': 'asma',\n",
    "        'asma exacerbation': 'asma',\n",
    "        'asma': 'asma',\n",
    "        'pemburukan asma': 'asma',\n",
    "        \n",
    "        # Bronchitis related\n",
    "        'bronkitis akut': 'bronkitis',\n",
    "        'bronkitis': 'bronkitis',\n",
    "        \n",
    "        # Wound related\n",
    "        'vulnus laceratum': 'luka robek',\n",
    "        'luka robek': 'luka robek',\n",
    "        'luka terbuka': 'luka robek',\n",
    "        'laceration': 'luka robek',\n",
    "        'vulnus excoriatum': 'luka lecet',\n",
    "        'luka lecet': 'luka lecet',\n",
    "        \n",
    "        # Head injury related\n",
    "        'cedera kepala ringan': 'ckr',\n",
    "        'kepala cedera ringan': 'ckr',\n",
    "        'ckr': 'ckr',\n",
    "        \n",
    "        # Dyspepsia related\n",
    "        'dispepsia': 'dispepsia',\n",
    "        'dispepsia fungsional': 'dispepsia',\n",
    "        \n",
    "        # Appendicitis related\n",
    "        'appendisitis akut': 'appendisitis',\n",
    "        'appendisitis': 'appendisitis',\n",
    "        'apendisitis': 'appendisitis',\n",
    "        \n",
    "        # UTI related\n",
    "        'infeksi saluran kemih': 'isk',\n",
    "        'isk': 'isk',\n",
    "        'infeksi saluran kemih akut': 'isk',\n",
    "        \n",
    "        # Additional mappings\n",
    "        'intoleransi laktosa': 'intoleransi laktosa',\n",
    "        'pneumonia': 'pneumonia',\n",
    "        'hipertensi': 'hipertensi',\n",
    "        'hipertensi akut': 'hipertensi',\n",
    "        'hipertensi stage 1': 'hipertensi',\n",
    "        'luka bakar': 'luka bakar',\n",
    "        'burn injury': 'luka bakar',\n",
    "        'tonsilitis': 'tonsilitis',\n",
    "        'faringitis': 'faringitis',\n",
    "        'vertigo': 'vertigo',\n",
    "        'benign paroxysmal positional vertigo': 'vertigo',\n",
    "        'bppv': 'vertigo',\n",
    "        'influenza': 'influenza',\n",
    "        'kolik renal': 'kolik renal',\n",
    "        'batu ginjal': 'batu ginjal',\n",
    "        'peritonitis': 'peritonitis'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping if available - using partial matching for more flexibility\n",
    "    for key, value in mapping.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    \n",
    "    # If no mapping found, return the cleaned name\n",
    "    return name\n",
    "\n",
    "# Function to extract percentage from diagnosis text\n",
    "def extract_percentage(text):\n",
    "    match = re.search(r'(\\d+)%', text)\n",
    "    return float(match.group(1))/100 if match else 0.5  # Default to 50% if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd8ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse diagnoses from raw answer text\n",
    "def parse_diagnoses(answer_text):\n",
    "    if pd.isna(answer_text):\n",
    "        return []\n",
    "    \n",
    "    # Split by semicolon and process each diagnosis\n",
    "    diagnoses = []\n",
    "    percentages = []\n",
    "    \n",
    "    for item in answer_text.split(';'):\n",
    "        item = item.strip()\n",
    "        if not item:\n",
    "            continue\n",
    "            \n",
    "        # Extract diagnosis name and percentage \n",
    "        match = re.search(r'(.*?)(?:\\s+(\\d+)%)?$', item)\n",
    "        if match and match.group(1).strip():\n",
    "            diagnoses.append(standardize_disease_name(match.group(1).strip()))\n",
    "            percentages.append(extract_percentage(item))\n",
    "    \n",
    "    # Create a list of tuples (diagnosis, percentage)\n",
    "    return list(zip(diagnoses, percentages))\n",
    "\n",
    "# Main processing function to analyze and evaluate diagnoses\n",
    "def process_medical_diagnoses():\n",
    "    # Load Excel file with both models' outputs\n",
    "    xlsx_path = '30 sample penyakit - hasil prompt LLM.xlsx'\n",
    "    \n",
    "    try:\n",
    "        # Load raw data from both sheets\n",
    "        claude_raw = pd.read_excel(xlsx_path, sheet_name='Claude 3.5 Haiku')\n",
    "        qwen_raw = pd.read_excel(xlsx_path, sheet_name='Qwen 2.5 72B')\n",
    "        \n",
    "        # Fill forward the question IDs and questions\n",
    "        claude_raw['No'] = claude_raw['No'].ffill()\n",
    "        claude_raw['Question'] = claude_raw['Question'].ffill()\n",
    "        qwen_raw['No'] = qwen_raw['No'].ffill()\n",
    "        qwen_raw['Question'] = qwen_raw['Question'].ffill()\n",
    "        \n",
    "        # Group by question to organize multiple answers per question\n",
    "        claude_grouped = claude_raw.groupby(['No', 'Question'])\n",
    "        qwen_grouped = qwen_raw.groupby(['No', 'Question'])\n",
    "        \n",
    "        # Create the unified question list - ensure we have all questions\n",
    "        all_questions = list(set(claude_grouped.groups.keys()) | set(qwen_grouped.groups.keys()))\n",
    "        all_questions.sort(key=lambda x: float(x[0]) if x[0] is not None and not pd.isna(x[0]) else float('inf'))\n",
    "        \n",
    "        # Results container\n",
    "        results = []\n",
    "        \n",
    "        # Process each question\n",
    "        for idx, (no, question) in enumerate(all_questions):\n",
    "            try:\n",
    "                print(f\"Processing question {idx+1}/{len(all_questions)}: {no}\")\n",
    "                \n",
    "                # Get Claude answers (ground truth)\n",
    "                if (no, question) in claude_grouped.groups:\n",
    "                    claude_rows = claude_grouped.get_group((no, question))\n",
    "                    claude_answers = claude_rows['Answer'].tolist()\n",
    "                    claude_diagnoses = []\n",
    "                    \n",
    "                    # Parse all Claude answers to get diagnoses with percentages\n",
    "                    for ans in claude_answers:\n",
    "                        if not pd.isna(ans):\n",
    "                            claude_diagnoses.extend(parse_diagnoses(ans))\n",
    "                    \n",
    "                    # Get the full text explanations\n",
    "                    claude_explanations = [text for text in claude_rows['Full Answer'].tolist() if not pd.isna(text)]\n",
    "                    claude_full_text = ' '.join(claude_explanations)\n",
    "                else:\n",
    "                    # Skip if no ground truth available\n",
    "                    print(f\"Warning: No Claude data for question {no}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get Qwen candidate answers\n",
    "                if (no, question) in qwen_grouped.groups:\n",
    "                    qwen_rows = qwen_grouped.get_group((no, question))\n",
    "                    qwen_answers = [ans for ans in qwen_rows['Answer'].tolist() if not pd.isna(ans)]\n",
    "                    \n",
    "                    # Select the best matching Qwen answer\n",
    "                    best_qwen, similarity_score = select_best_qwen_answer(qwen_answers, claude_diagnoses)\n",
    "                    \n",
    "                    # Parse the best Qwen answer\n",
    "                    qwen_diagnoses = parse_diagnoses(best_qwen)\n",
    "                    \n",
    "                    # Get Qwen full text matching the best answer\n",
    "                    qwen_explanation_rows = qwen_rows[qwen_rows['Answer'] == best_qwen]\n",
    "                    qwen_explanations = [text for text in qwen_explanation_rows['Full Answer'].tolist() if not pd.isna(text)]\n",
    "                    qwen_full_text = ' '.join(qwen_explanations) if qwen_explanations else \"\"\n",
    "                else:\n",
    "                    print(f\"Warning: No Qwen data for question {no}\")\n",
    "                    best_qwen = \"\"\n",
    "                    similarity_score = 0.0\n",
    "                    qwen_diagnoses = []\n",
    "                    qwen_full_text = \"\"\n",
    "                \n",
    "                # Calculate diagnosis match metrics\n",
    "                diagnosis_metrics = calculate_diagnosis_match_metrics(claude_diagnoses, qwen_diagnoses)\n",
    "                \n",
    "                # Calculate NLP similarity metrics\n",
    "                # For answer (diagnosis names only)\n",
    "                claude_answer_text = '; '.join([f\"{d} {int(p*100)}%\" for d, p in claude_diagnoses])\n",
    "                \n",
    "                nlp_metrics = {\n",
    "                    'answer_cosine_similarity': calculate_cosine_similarity(\n",
    "                        preprocess_text(claude_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    ),\n",
    "                    'full_answer_cosine_similarity': calculate_cosine_similarity(\n",
    "                        preprocess_text(claude_full_text), \n",
    "                        preprocess_text(qwen_full_text)\n",
    "                    ),\n",
    "                    'bleu_score': calculate_bleu_score(\n",
    "                        preprocess_text(claude_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    ),\n",
    "                    'meteor_score': calculate_meteor_score(\n",
    "                        preprocess_text(claude_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    ),\n",
    "                    'bert_score': calculate_bert_score(\n",
    "                        preprocess_text(claude_answer_text), \n",
    "                        preprocess_text(best_qwen)\n",
    "                    )\n",
    "                }\n",
    "                \n",
    "                # Combined all metrics into one result entry\n",
    "                result = {\n",
    "                    'No': no,\n",
    "                    'Question': question,\n",
    "                    'Claude_Diagnoses': claude_answer_text,\n",
    "                    'Selected_Qwen_Answer': best_qwen,\n",
    "                    'Similarity_Score': similarity_score,\n",
    "                    **diagnosis_metrics,\n",
    "                    **nlp_metrics\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing question {no}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Create final results dataframe\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        overall_metrics = {\n",
    "            'total_questions': len(results),\n",
    "            'average_precision': results_df['precision'].mean(),\n",
    "            'average_recall': results_df['recall'].mean(),\n",
    "            'average_f1_score': results_df['f1_score'].mean(),\n",
    "            'average_answer_cosine_similarity': results_df['answer_cosine_similarity'].mean(),\n",
    "            'average_full_answer_cosine_similarity': results_df['full_answer_cosine_similarity'].mean(),\n",
    "            'average_bleu_score': results_df['bleu_score'].mean(),\n",
    "            'average_meteor_score': results_df['meteor_score'].mean(),\n",
    "            'average_bert_score': results_df['bert_score'].mean()\n",
    "        }\n",
    "        \n",
    "        return results_df, overall_metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel file: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41068d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the best matching Qwen answer for a given Claude answer\n",
    "def select_best_qwen_answer(qwen_answers, claude_diagnoses):\n",
    "    \"\"\"\n",
    "    Selects the best matching Qwen answer based on overlap with Claude diagnoses\n",
    "    \n",
    "    Args:\n",
    "        qwen_answers: List of Qwen answer strings\n",
    "        claude_diagnoses: List of (diagnosis, score) tuples from Claude\n",
    "    \n",
    "    Returns:\n",
    "        best_answer: The Qwen answer with highest similarity score\n",
    "    \"\"\"\n",
    "    if not qwen_answers or not claude_diagnoses:\n",
    "        return \"\"\n",
    "    \n",
    "    best_score = -1\n",
    "    best_answer = \"\"\n",
    "    \n",
    "    claude_diseases = [d[0] for d in claude_diagnoses]\n",
    "    \n",
    "    for qwen_ans in qwen_answers:\n",
    "        # Parse the current Qwen answer\n",
    "        qwen_parsed = parse_diagnoses(qwen_ans)\n",
    "        qwen_diseases = [d[0] for d in qwen_parsed]\n",
    "        \n",
    "        # Calculate overlap score - how many diseases match\n",
    "        match_count = sum(1 for d in qwen_diseases if d in claude_diseases)\n",
    "        match_score = match_count / max(len(claude_diseases), len(qwen_diseases), 1)\n",
    "        \n",
    "        # Also consider text similarity as fallback\n",
    "        text_sim = calculate_cosine_similarity(\n",
    "            preprocess_text(qwen_ans), \n",
    "            preprocess_text('; '.join([d for d, _ in claude_diagnoses]))\n",
    "        )\n",
    "        \n",
    "        # Combined score favoring matching diseases but also considering text similarity\n",
    "        combined_score = (match_score * 0.7) + (text_sim * 0.3)\n",
    "        \n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_answer = qwen_ans\n",
    "            \n",
    "    return best_answer, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4daec43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate diagnosis matching metrics\n",
    "def calculate_diagnosis_match_metrics(claude_diagnoses, qwen_diagnoses):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for disease matching\n",
    "    \n",
    "    Args:\n",
    "        claude_diagnoses: List of (disease, score) tuples from Claude\n",
    "        qwen_diagnoses: List of (disease, score) tuples from Qwen\n",
    "    \n",
    "    Returns:\n",
    "        dict with precision, recall, f1_score, etc.\n",
    "    \"\"\"\n",
    "    if not claude_diagnoses or not qwen_diagnoses:\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'matched': 0,\n",
    "            'total_ground_truth': len(claude_diagnoses),\n",
    "            'total_predictions': len(qwen_diagnoses)\n",
    "        }\n",
    "    \n",
    "    # Extract just the disease names\n",
    "    claude_diseases = [d[0] for d in claude_diagnoses]\n",
    "    qwen_diseases = [d[0] for d in qwen_diagnoses]\n",
    "    \n",
    "    # Count matches\n",
    "    matches = sum(1 for d in qwen_diseases if d in claude_diseases)\n",
    "    \n",
    "    # Calculate precision: matches / qwen predictions\n",
    "    precision = matches / len(qwen_diseases) if qwen_diseases else 0\n",
    "    \n",
    "    # Calculate recall: matches / claude diagnoses (ground truth)\n",
    "    recall = matches / len(claude_diseases) if claude_diseases else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'matched': matches,\n",
    "        'total_ground_truth': len(claude_diseases),\n",
    "        'total_predictions': len(qwen_diseases)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86ee91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting medical diagnosis evaluation...\n",
      "Processing question 1/46: 1.0\n",
      "Processing question 2/46: 2.0\n",
      "Processing question 3/46: 3.0\n",
      "Processing question 4/46: 4.0\n",
      "Processing question 5/46: 5.0\n",
      "Processing question 6/46: 6.0\n",
      "Processing question 7/46: 7.0\n",
      "Warning: No Qwen data for question 7.0\n",
      "Processing question 8/46: 7.0\n",
      "Warning: No Claude data for question 7.0\n",
      "Processing question 9/46: 8.0\n",
      "Processing question 10/46: 9.0\n",
      "Processing question 11/46: 10.0\n",
      "Processing question 12/46: 11.0\n",
      "Processing question 13/46: 12.0\n",
      "Processing question 14/46: 13.0\n",
      "Processing question 15/46: 14.0\n",
      "Processing question 16/46: 15.0\n",
      "Processing question 17/46: 16.0\n",
      "Processing question 18/46: 17.0\n",
      "Processing question 19/46: 18.0\n",
      "Processing question 20/46: 19.0\n",
      "Processing question 21/46: 20.0\n",
      "Processing question 22/46: 21.0\n",
      "Processing question 23/46: 22.0\n",
      "Processing question 24/46: 23.0\n",
      "Processing question 25/46: 24.0\n",
      "Processing question 26/46: 25.0\n",
      "Processing question 27/46: 26.0\n",
      "Processing question 28/46: 27.0\n",
      "Processing question 29/46: 28.0\n",
      "Processing question 30/46: 29.0\n",
      "Processing question 31/46: 30.0\n",
      "Processing question 32/46: 31.0\n",
      "Processing question 33/46: 32.0\n",
      "Processing question 34/46: 33.0\n",
      "Processing question 35/46: 34.0\n",
      "Processing question 36/46: 35.0\n",
      "Processing question 37/46: 36.0\n",
      "Processing question 38/46: 37.0\n",
      "Processing question 39/46: 38.0\n",
      "Processing question 40/46: 39.0\n",
      "Processing question 41/46: 40.0\n",
      "Processing question 42/46: 41.0\n",
      "Processing question 43/46: 42.0\n",
      "Processing question 44/46: 43.0\n",
      "Processing question 45/46: 44.0\n",
      "Processing question 46/46: 45.0\n",
      "\n",
      "=== OVERALL EVALUATION METRICS ===\n",
      "Total Questions: 45.0000\n",
      "Average Precision: 0.7778\n",
      "Average Recall: 0.3926\n",
      "Average F1 Score: 0.5074\n",
      "Average Answer Cosine Similarity: 0.3470\n",
      "Average Full Answer Cosine Similarity: 0.2707\n",
      "Average Bleu Score: 0.1485\n",
      "Average Meteor Score: 0.2462\n",
      "Average Bert Score: 0.8331\n",
      "\n",
      "=== TOP 5 QUESTIONS WITH HIGHEST F1 SCORES ===\n",
      "Question 3.0: F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Claude: demam berdarah dengue 80%\n",
      "  Qwen: Demam Dengue\n",
      "\n",
      "Question 29.0: F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Claude: luka lecet 100%\n",
      "  Qwen: Luka Lecet\n",
      "\n",
      "Question 26.0: F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Claude: luka robek 100%\n",
      "  Qwen: Luka Robek (Laceration)\n",
      "\n",
      "Question 20.0: F1 = 1.0000, Precision = 1.0000, Recall = 1.0000\n",
      "  Claude: ispa 80%\n",
      "  Qwen: Infeksi Saluran Pernapasan Atas (ISPA)\n",
      "\n",
      "Question 5.0: F1 = 0.6667, Precision = 1.0000, Recall = 0.5000\n",
      "  Claude: angina 80%; serangan jantung 80%\n",
      "  Qwen: Infark Miokard Akut (Serangan Jantung)\n",
      "\n",
      "\n",
      "=== QUESTIONS WITH LOWEST F1 SCORES ===\n",
      "Question 1.0: F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Claude: demam tifoid 70%; demam berdarah dengue 30%\n",
      "  Qwen: Demam Biasa\n",
      "\n",
      "Question 4.0: F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Claude: infeksi bakteri/virus (sistemik) 50%; infeksi jaringan lunak (selulitis) 70%\n",
      "  Qwen: Infeksi Lokal\n",
      "\n",
      "Question 7.0: F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Claude: bronkitis 100%\n",
      "  Qwen: \n",
      "\n",
      "Question 12.0: F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Claude: batu ginjal 70%; kolik renal 80%; ganguan saluran kemih 100%; ureterolitiasis 70%\n",
      "  Qwen: Infeksi Saluran Kemih (ISK) Akut\n",
      "\n",
      "Question 28.0: F1 = 0.0000, Precision = 0.0000, Recall = 0.0000\n",
      "  Claude: ckr 90%; luka robek 100%; gangguan keseimbangan neurologis lain 70%\n",
      "  Qwen: Benign Paroxysmal Positional Vertigo (BPPV)\n",
      "\n",
      "\n",
      "Results saved to medical_diagnosis_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to run the full evaluation and display results\n",
    "def main():\n",
    "    print(\"Starting medical diagnosis evaluation...\")\n",
    "    results_df, overall_metrics = process_medical_diagnoses()\n",
    "    \n",
    "    if results_df is not None:\n",
    "        # Display summary statistics\n",
    "        print(\"\\n=== OVERALL EVALUATION METRICS ===\")\n",
    "        for metric, value in overall_metrics.items():\n",
    "            print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        \n",
    "        # Display per-question results\n",
    "        print(\"\\n=== TOP 5 QUESTIONS WITH HIGHEST F1 SCORES ===\")\n",
    "        top_questions = results_df.sort_values('f1_score', ascending=False).head(5)\n",
    "        for _, row in top_questions.iterrows():\n",
    "            print(f\"Question {row['No']}: F1 = {row['f1_score']:.4f}, Precision = {row['precision']:.4f}, Recall = {row['recall']:.4f}\")\n",
    "            print(f\"  Claude: {row['Claude_Diagnoses']}\")\n",
    "            print(f\"  Qwen: {row['Selected_Qwen_Answer']}\")\n",
    "            print()\n",
    "        \n",
    "        # Display questions with lowest scores\n",
    "        print(\"\\n=== QUESTIONS WITH LOWEST F1 SCORES ===\")\n",
    "        bottom_questions = results_df.sort_values('f1_score').head(5)\n",
    "        for _, row in bottom_questions.iterrows():\n",
    "            print(f\"Question {row['No']}: F1 = {row['f1_score']:.4f}, Precision = {row['precision']:.4f}, Recall = {row['recall']:.4f}\")\n",
    "            print(f\"  Claude: {row['Claude_Diagnoses']}\")\n",
    "            print(f\"  Qwen: {row['Selected_Qwen_Answer']}\")\n",
    "            print()\n",
    "        \n",
    "        # Save results to CSV \n",
    "        results_df.to_csv('medical_diagnosis_evaluation_results.csv', index=False)\n",
    "        print(\"\\nResults saved to medical_diagnosis_evaluation_results.csv\")\n",
    "    else:\n",
    "        print(\"Error: Evaluation failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ef4f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnalysis of Low Similarity Scores in Medical Diagnosis Evaluation\\n\\nSummary of Scores\\nFrom the evaluation results, we see:\\n- Average Precision: 0.7778 (good)\\n- Average Recall: 0.3926 (low)\\n- Average F1 Score: 0.5074 (moderate)\\n- Text-based metrics:\\n  - Answer Cosine Similarity: 0.3470 (low)\\n  - BLEU Score: 0.1485 (very low)\\n  - METEOR Score: 0.2462 (low)\\n  - BERT Score: 0.8331 (good)\\n\\nWhy Are Some Scores Low?\\n1. Limitations of Text-Based Metrics\\n\\nText similarity metrics like BLEU, METEOR, and cosine similarity have inherent limitations for this task:\\n\\n- BLEU (0.1485) measures n-gram overlap, which is very sensitive to word order and exact matches\\n- METEOR (0.2462) performs better but still depends on exact or stemmed word matches\\n- Cosine similarity (0.3470) works with bag-of-words but loses semantic relationships\\n\\nOnly BERT Score (0.8331) performs well because it\\'s based on contextual embeddings that capture semantic meaning.\\n\\n2. Medical Terminology Variation\\n\\nEven with improved standardization, medical terms can be expressed in many ways:\\n\\n```\\n# Example from Question 28.0:\\nGround Truth: \"ckr 90%; luka robek 100%; gangguan keseimbangan neurologis lain 70%\"\\nQwen: \"Benign Paroxysmal Positional Vertigo (BPPV)\"\\n```\\n\\nBPPV is a specific type of neurological balance disorder, but our mapping doesn\\'t capture this relationship.\\n\\n3. Multilingual Context\\n\\nThe dataset contains a mix of Indonesian and English medical terms:\\n- \"demam berdarah dengue\" vs. \"dengue fever\"\\n- \"luka robek\" vs. \"laceration\"\\n\\nThese variations make text-based matching more challenging.\\n\\n## Suggestions for Improving Scores\\n\\n1. **Enhanced disease mapping**:\\n   - Add more semantic relationships between medical conditions\\n   - Map \"BPPV\" to \"gangguan keseimbangan neurologis\"\\n   - Map \"Infeksi Lokal\" to various infection types\\n\\n2. **Weighted evaluation metrics**:\\n   - Consider the confidence scores when calculating matches\\n   - A diagnosis with 80% confidence should count more than one with 30%\\n\\n3. **Domain-specific evaluation**:\\n   - Develop a medical ontology-based similarity metric\\n   - Use hierarchical relationships (e.g., BPPV is a subtype of balance disorder)\\n\\n4. **Improve dataset alignment**:\\n   - Fix question format inconsistencies\\n   - Ensure both models have answers for all questions\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analysis of Low Similarity Scores in Medical Diagnosis Evaluation\n",
    "\n",
    "Summary of Scores\n",
    "From the evaluation results, we see:\n",
    "- Average Precision: 0.7778 (good)\n",
    "- Average Recall: 0.3926 (low)\n",
    "- Average F1 Score: 0.5074 (moderate)\n",
    "- Text-based metrics:\n",
    "  - Answer Cosine Similarity: 0.3470 (low)\n",
    "  - BLEU Score: 0.1485 (very low)\n",
    "  - METEOR Score: 0.2462 (low)\n",
    "  - BERT Score: 0.8331 (good)\n",
    "\n",
    "Why Are Some Scores Low?\n",
    "1. Limitations of Text-Based Metrics\n",
    "\n",
    "Text similarity metrics like BLEU, METEOR, and cosine similarity have inherent limitations for this task:\n",
    "\n",
    "- BLEU (0.1485) measures n-gram overlap, which is very sensitive to word order and exact matches\n",
    "- METEOR (0.2462) performs better but still depends on exact or stemmed word matches\n",
    "- Cosine similarity (0.3470) works with bag-of-words but loses semantic relationships\n",
    "\n",
    "Only BERT Score (0.8331) performs well because it's based on contextual embeddings that capture semantic meaning.\n",
    "\n",
    "2. Medical Terminology Variation\n",
    "\n",
    "Even with improved standardization, medical terms can be expressed in many ways:\n",
    "\n",
    "```\n",
    "# Example from Question 28.0:\n",
    "Ground Truth: \"ckr 90%; luka robek 100%; gangguan keseimbangan neurologis lain 70%\"\n",
    "Qwen: \"Benign Paroxysmal Positional Vertigo (BPPV)\"\n",
    "```\n",
    "\n",
    "BPPV is a specific type of neurological balance disorder, but our mapping doesn't capture this relationship.\n",
    "\n",
    "3. Multilingual Context\n",
    "\n",
    "The dataset contains a mix of Indonesian and English medical terms:\n",
    "- \"demam berdarah dengue\" vs. \"dengue fever\"\n",
    "- \"luka robek\" vs. \"laceration\"\n",
    "\n",
    "These variations make text-based matching more challenging.\n",
    "\n",
    "## Suggestions for Improving Scores\n",
    "\n",
    "1. **Enhanced disease mapping**:\n",
    "   - Add more semantic relationships between medical conditions\n",
    "   - Map \"BPPV\" to \"gangguan keseimbangan neurologis\"\n",
    "   - Map \"Infeksi Lokal\" to various infection types\n",
    "\n",
    "2. **Weighted evaluation metrics**:\n",
    "   - Consider the confidence scores when calculating matches\n",
    "   - A diagnosis with 80% confidence should count more than one with 30%\n",
    "\n",
    "3. **Domain-specific evaluation**:\n",
    "   - Develop a medical ontology-based similarity metric\n",
    "   - Use hierarchical relationships (e.g., BPPV is a subtype of balance disorder)\n",
    "\n",
    "4. **Improve dataset alignment**:\n",
    "   - Fix question format inconsistencies\n",
    "   - Ensure both models have answers for all questions\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
